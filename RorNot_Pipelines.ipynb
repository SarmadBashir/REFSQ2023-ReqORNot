{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from typing import Dict\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import glob\n",
    "import torch\n",
    "import wget\n",
    "import zipfile\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "from glove import Glove\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import load_model\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from datasets import load_dataset, logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import XLNetForSequenceClassification, RobertaForSequenceClassification\n",
    "from transformers import XLMRobertaForSequenceClassification, DistilBertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, XLMRobertaTokenizer, DistilBertTokenizer, XLNetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_report(results, folds):\n",
    "    \n",
    "    \"\"\"\n",
    "    function takes the input of predicted model results on five folds and returns\n",
    "    average of weighted and macro Precision, Recall, F-1 \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    weighted_precision = []\n",
    "    weighted_recall = []\n",
    "    weighted_f1 = []\n",
    "    \n",
    "    macro_precision = []\n",
    "    macro_recall = []\n",
    "    macro_f1 = []\n",
    "    \n",
    "    for result_df in results:                        \n",
    "        res_rows = result_df.tail(3)\n",
    "\n",
    "        precision_scores =  res_rows['precision'].tolist()\n",
    "        recall_scores =  res_rows['recall'].tolist()\n",
    "        f1_scores =  res_rows['f1-score'].tolist()\n",
    "\n",
    "        precision_macro_avg =  precision_scores[1]\n",
    "        precision_weighted_avg = precision_scores[2]\n",
    "\n",
    "        recall_macro_avg =  recall_scores[1]\n",
    "        recall_weighted_avg = recall_scores[2]\n",
    "\n",
    "        fl_accuracy = f1_scores[0]\n",
    "        f1_scores_macro_avg =  f1_scores[1]\n",
    "        f1_scores_weighted_avg = f1_scores[2]\n",
    "                \n",
    "        weighted_precision.append(precision_weighted_avg)\n",
    "        weighted_recall.append(recall_weighted_avg)\n",
    "        weighted_f1.append(f1_scores_weighted_avg)\n",
    "        \n",
    "        macro_precision.append(precision_macro_avg)\n",
    "        macro_recall.append(recall_macro_avg)\n",
    "        macro_f1.append(f1_scores_macro_avg)\n",
    "                \n",
    "    weighted_average = round(sum(weighted_precision) / folds, 2), round(sum(weighted_recall) / folds, 2), round(sum(weighted_f1) / folds, 2)\n",
    "    macro_average = round(sum(macro_precision) / folds, 2), round(sum(macro_recall) / folds, 2), round(sum(macro_f1) / folds, 2)\n",
    "            \n",
    "    return weighted_average, macro_average\n",
    "\n",
    "def get_accuracy(y_actual, y_predicted):\n",
    "    \"\"\"\n",
    "    function takes the actual and predicted labels to return\n",
    "    the accuracy per fold\n",
    "    \n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for index in zip(y_actual, y_predicted):\n",
    "        \n",
    "        if index[0] == index[1]:\n",
    "                count += 1\n",
    "    topk_acc = round(count / len(y_actual), 2)\n",
    "    return topk_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML alogrithms Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ML_model_files(model_name, model_path, pca):\n",
    "    \n",
    "    \"\"\"\n",
    "    function load the ML models relevant files based \n",
    "    on the parameters given\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ML_model = pickle.load(open(model_path + '/'+ model_name + '.pickle', 'rb'))\n",
    "    if pca:\n",
    "        pca_vectorizer = pickle.load(open(model_path + 'pca_vectorizer.pickle', \"rb\"))\n",
    "    else:\n",
    "        pca_vectorizer = None\n",
    "    tfidf_vectorizer = pickle.load(open(model_path + 'tfidf_vectorizer.pickle', \"rb\"))\n",
    "    \n",
    "    return ML_model, pca_vectorizer, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset for testing\n",
    "fold_parent = './data/dronology_five_folds/'\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the value of 'model_name' with desired tradional ML model's name to get resuts for the model\n",
    "# to trigger more traditional ML models check the names in: model/ML_models. examples, DT, SVM, pLR etc. \n",
    "# put 'p' infront of the model name to couple our pre-processing pipeline\n",
    "model_name = 'SVM'\n",
    "PCA = True\n",
    "map_labels = {0: 'information', 1: 'requirement'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 1 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.823529  1.000000  0.903226  56.000000\n",
      "requirement    1.000000  0.400000  0.571429  20.000000\n",
      "accuracy       0.842105  0.842105  0.842105   0.842105\n",
      "macro avg      0.911765  0.700000  0.737327  76.000000\n",
      "weighted avg   0.869969  0.842105  0.815911  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 2 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.787879  0.928571  0.852459  56.000000\n",
      "requirement    0.600000  0.300000  0.400000  20.000000\n",
      "accuracy       0.763158  0.763158  0.763158   0.763158\n",
      "macro avg      0.693939  0.614286  0.626230  76.000000\n",
      "weighted avg   0.738437  0.763158  0.733391  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 3 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score  support\n",
      "information    0.768116  0.963636  0.854839    55.00\n",
      "requirement    0.666667  0.200000  0.307692    20.00\n",
      "accuracy       0.760000  0.760000  0.760000     0.76\n",
      "macro avg      0.717391  0.581818  0.581266    75.00\n",
      "weighted avg   0.741063  0.760000  0.708933    75.00\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 4 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.805970  0.981818  0.885246  55.000000\n",
      "requirement    0.875000  0.350000  0.500000  20.000000\n",
      "accuracy       0.813333  0.813333  0.813333   0.813333\n",
      "macro avg      0.840485  0.665909  0.692623  75.000000\n",
      "weighted avg   0.824378  0.813333  0.782514  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 5 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.776119  0.928571  0.845528  56.000000\n",
      "requirement    0.500000  0.210526  0.296296  19.000000\n",
      "accuracy       0.746667  0.746667  0.746667   0.746667\n",
      "macro avg      0.638060  0.569549  0.570912  75.000000\n",
      "weighted avg   0.706169  0.746667  0.706390  75.000000\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# load test data & make prediction\n",
    "\n",
    "ml_results = []\n",
    "avg_accuracy = []\n",
    "fold_count = 1\n",
    "\n",
    "for subs in sorted(sub_folders):\n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "    \n",
    "    df_test = pd.read_csv(test_path)\n",
    "    df_test['STR.REQ'] = df_test['STR.REQ'].str.lower()\n",
    "    X_test = df_test['STR.REQ']\n",
    "    y_test = df_test['class']\n",
    "    \n",
    "    model_path = './models/ML_models/' + model_name + '/fold_' + str(fold_count) + '/'\n",
    "    ML_model, pca_vectorizer, tfidf_vectorizer = load_ML_model_files(model_name, model_path, PCA)\n",
    "\n",
    "    tfidf_vecs = tfidf_vectorizer.transform(X_test)\n",
    "    normalized_tfidf = normalize(tfidf_vecs)\n",
    "\n",
    "    test_vecs = pca_vectorizer.transform(normalized_tfidf.toarray())\n",
    "    predicted_labels = ML_model.predict(test_vecs)\n",
    "    \n",
    "    evaluation_results = classification_report(y_test.tolist(), predicted_labels.tolist(), \n",
    "                                               target_names=list(map_labels.values()), \n",
    "                                               output_dict=True)\n",
    "    \n",
    "    avg_accuracy.append(get_accuracy(y_test.tolist(), predicted_labels.tolist()))\n",
    "    \n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    ml_results.append(report_df)\n",
    "    \n",
    "    print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "    \n",
    "    fold_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5-folds</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weighted_avg</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro_avg</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_avg</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Precision  Recall  F1_score\n",
       "5-folds                                  \n",
       "weighted_avg       0.78    0.79      0.75\n",
       "macro_avg          0.76    0.63      0.64\n",
       "accuracy_avg       0.78    0.78      0.78"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average results of ML pipeline\n",
    "\n",
    "avg_acc_score = round(np.mean(avg_accuracy), 2)\n",
    "weighted_avg, macro_avg = get_avg_report(ml_results, folds=5)\n",
    "\n",
    "avg_scores = list([weighted_avg, macro_avg, (avg_acc_score, avg_acc_score, avg_acc_score)])\n",
    "\n",
    "final_df = pd.DataFrame([x for x in avg_scores], columns=(['Precision', 'Recall', 'F1_score']),\n",
    "                      index=['weighted_avg','macro_avg', 'accuracy_avg'])\n",
    "\n",
    "final_df.rename_axis('5-folds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Family Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    loads and returns the relevant tokenizer for passed parameter BERT model name\n",
    "    \n",
    "    \"\"\"\n",
    "    if model_name in ('BERT_base_uncased', \n",
    "                      'pBERT_base_uncased'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",\n",
    "                                                  do_lower_case=True)\n",
    "                \n",
    "    elif model_name in ('BERT_base_cased',\n",
    "                        'pBERT_base_cased'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    \n",
    "    elif model_name in ('pXLNet_base', \n",
    "                        'XLNet_base'):\n",
    "        tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "    \n",
    "    elif model_name in ('SciBERT_uncased', \n",
    "                        'pSciBERT_uncased'):\n",
    "        tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', \n",
    "                                                  do_lower_case=True)\n",
    "    \n",
    "    elif model_name in ('pRoBERTa_base', \n",
    "                        'RoBERTa_base'):\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "    elif model_name in ('DisBERT_base_cased', \n",
    "                        'pDisBERT_base_cased'):\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "    \n",
    "    elif model_name in ('DisBERT_base_uncased', \n",
    "                        'pDisBERT_base_uncased'):\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    else:\n",
    "        #'pXRBERT_base', 'XRBERT_base'\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    \n",
    "    return tokenizer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_BERT_model(model_name, model_path):\n",
    "    \"\"\"\n",
    "    loads and returns the BERT model based on the model name and path parameters\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if model_name in ('BERT_base_uncased', 'pBERT_base_cased',\n",
    "                      'pBERT_base_uncased', 'BERT_base_cased',\n",
    "                      'SciBERT_uncased', 'pSciBERT_uncased'\n",
    "                     ):\n",
    "        model = BertForSequenceClassification.from_pretrained(model_path)                \n",
    "    elif model_name in ('pXLNet_base', \n",
    "                        'XLNet_base'\n",
    "                       ):\n",
    "        model = XLNetForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    elif model_name in ('pRoBERTa_base', \n",
    "                        'RoBERTa_base'\n",
    "                       ):\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    elif model_name in ('DisBERT_base_cased', 'DisBERT_base_uncased',\n",
    "                        'pDisBERT_base_cased', 'pDisBERT_base_uncased'\n",
    "                       ):\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_path)    \n",
    "    \n",
    "    else:\n",
    "        #'pXRBERT_base', 'XRBERT_base'\n",
    "        model = XLMRobertaForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the value of 'model_name' with BERT model's name to get resuts for the model\n",
    "# to trigger more BERT models check the names in: model/BERT_family. examples, BERT_base_cased etc. \n",
    "# put 'p' infront of the model name to couple our pre-processing pipeline\n",
    "\n",
    "map_labels = {0: 'information', 1: 'requirement'}\n",
    "\n",
    "prefix = './models/DL_models/BERT_family/'\n",
    "model_name = 'DisBERT_base_uncased'\n",
    "\n",
    "fold_parent = './data/dronology_five_folds/'\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder))\n",
    "\n",
    "tokenizer = load_tokenizer(model_name)\n",
    "MAX_SEQ_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 1 on model : DisBERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.981132  0.928571  0.954128  56.000000\n",
      "requirement    0.826087  0.950000  0.883721  20.000000\n",
      "accuracy       0.934211  0.934211  0.934211   0.934211\n",
      "macro avg      0.903610  0.939286  0.918925  76.000000\n",
      "weighted avg   0.940331  0.934211  0.935600  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 2 on model : DisBERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.833333  0.892857  0.862069  56.000000\n",
      "requirement    0.625000  0.500000  0.555556  20.000000\n",
      "accuracy       0.789474  0.789474  0.789474   0.789474\n",
      "macro avg      0.729167  0.696429  0.708812  76.000000\n",
      "weighted avg   0.778509  0.789474  0.781408  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 3 on model : DisBERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score  support\n",
      "information    0.942308  0.890909  0.915888    55.00\n",
      "requirement    0.739130  0.850000  0.790698    20.00\n",
      "accuracy       0.880000  0.880000  0.880000     0.88\n",
      "macro avg      0.840719  0.870455  0.853293    75.00\n",
      "weighted avg   0.888127  0.880000  0.882504    75.00\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 4 on model : DisBERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.944444  0.927273  0.935780  55.000000\n",
      "requirement    0.809524  0.850000  0.829268  20.000000\n",
      "accuracy       0.906667  0.906667  0.906667   0.906667\n",
      "macro avg      0.876984  0.888636  0.882524  75.000000\n",
      "weighted avg   0.908466  0.906667  0.907377  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 5 on model : DisBERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.809524  0.910714  0.857143  56.000000\n",
      "requirement    0.583333  0.368421  0.451613  19.000000\n",
      "accuracy       0.773333  0.773333  0.773333   0.773333\n",
      "macro avg      0.696429  0.639568  0.654378  75.000000\n",
      "weighted avg   0.752222  0.773333  0.754409  75.000000\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_count = 1\n",
    "results = []\n",
    "avg_accuracy = []\n",
    "for subs in sorted(sub_folders):\n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "    \n",
    "    df_test = pd.read_csv(test_path)\n",
    "    selected_test = df_test[['STR.REQ','class']]\n",
    "\n",
    "    test_sequences = selected_test['STR.REQ'].tolist()\n",
    "\n",
    "    test_encodings = tokenizer(test_sequences, truncation=True, \n",
    "                               padding=True, \n",
    "                               max_length=MAX_SEQ_LENGTH, \n",
    "                               return_tensors=\"pt\")\n",
    "    # load model\n",
    "    model_path = glob.glob(prefix + model_name + '/fold_' + str(fold_count) + '/*')[0]\n",
    "    bert_model = load_BERT_model(model_name, model_path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = bert_model(**test_encodings).logits\n",
    "\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    evaluation_results = classification_report(selected_test['class'].tolist(), \n",
    "                                               predictions.tolist(), \n",
    "                                               target_names=list(map_labels.values()), \n",
    "                                               output_dict=True)\n",
    "    \n",
    "    avg_accuracy.append(get_accuracy(selected_test['class'].tolist(), \n",
    "                                     predictions.tolist()))\n",
    "\n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    results.append(report_df)\n",
    "    \n",
    "    print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "    \n",
    "    fold_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5-folds</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weighted_avg</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro_avg</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_avg</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Precision  Recall  F1_score\n",
       "5-folds                                  \n",
       "weighted_avg       0.85    0.86      0.85\n",
       "macro_avg          0.81    0.81      0.80\n",
       "accuracy_avg       0.86    0.86      0.86"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average results of BERT model\n",
    "\n",
    "avg_acc_score = round(np.mean(avg_accuracy), 2)\n",
    "weighted_avg, macro_avg = get_avg_report(results, folds=5)\n",
    "\n",
    "avg_scores = list([weighted_avg, macro_avg, (avg_acc_score, avg_acc_score, \n",
    "                                             avg_acc_score)])\n",
    "\n",
    "final_df = pd.DataFrame([x for x in avg_scores], \n",
    "                        columns=(['Precision', 'Recall', 'F1_score']),\n",
    "                        index=['weighted_avg','macro_avg', 'accuracy_avg'])\n",
    "\n",
    "final_df.rename_axis('5-folds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fewshot Family pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "    \n",
    "    \"\"\"\n",
    "    load and return the dataset in the format fine-tuned few shot sentence-BERT \n",
    "    expects\n",
    "    \n",
    "    \"\"\"\n",
    "    dataset = load_dataset(path)\n",
    "    test_dataset = dataset['test']\n",
    "    return test_dataset\n",
    "\n",
    "def _apply_column_mapping(dataset, column_mapping: Dict[str, str]):\n",
    "    \n",
    "    \"\"\"\n",
    "    apply the column mapping required for the loaded dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    dataset = dataset.rename_columns(\n",
    "        {\n",
    "            **column_mapping,\n",
    "            **{col: f\"feat_{col}\" for col in dataset.column_names if col not in column_mapping},\n",
    "        }\n",
    "    )\n",
    "    dset_format = dataset.format\n",
    "    dataset = dataset.with_format(\n",
    "        type=dset_format[\"type\"],\n",
    "        columns=dataset.column_names,\n",
    "        output_all_columns=dset_format[\"output_all_columns\"],\n",
    "        **dset_format[\"format_kwargs\"],\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def evaluate_ST(test_data, Sent_tf_model):\n",
    "    \n",
    "    \"\"\"\n",
    "    load and evaluate the Sentence-BERT model on the given test dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    eval_dataset = _apply_column_mapping(test_data, \n",
    "                                         column_mapping={\"STR.REQ\": \"text\", \"class\": \"label\"})   \n",
    "    x_test = eval_dataset[\"text\"]\n",
    "    y_test = eval_dataset[\"label\"]\n",
    "    predicted_labels = Sent_tf_model.predict(x_test)\n",
    "    \n",
    "    return predicted_labels, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_labels = {0: 'information', 1: 'requirement'}\n",
    "# replace the value of 'model_name' with desired few shot model's name to get resuts for the model\n",
    "# to trigger morefew shot models models check the names in: model/Fewshot_family. examples, S-BERT_10% or pMiniLM_10%. \n",
    "# put 'p' infront of the model name to couple our pre-processing pipeline\n",
    "model_name = 'pS-BERT_20%'\n",
    "\n",
    "prefix = './models/DL_models/Fewshot_family/'\n",
    "fold_parent = './data/dronology_preprocess_five_folds/'\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012858390808105469,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bc0b0bbacb4c5fb714f65ca3ecd670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 1 on model : pS-BERT_20%\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.840909  0.660714  0.740000  56.000000\n",
      "requirement    0.406250  0.650000  0.500000  20.000000\n",
      "accuracy       0.657895  0.657895  0.657895   0.657895\n",
      "macro avg      0.623580  0.655357  0.620000  76.000000\n",
      "weighted avg   0.726525  0.657895  0.676842  76.000000\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012038946151733398,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525ba12d013c4175aeb747c599058e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 2 on model : pS-BERT_20%\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.857143  0.642857  0.734694  56.000000\n",
      "requirement    0.411765  0.700000  0.518519  20.000000\n",
      "accuracy       0.657895  0.657895  0.657895   0.657895\n",
      "macro avg      0.634454  0.671429  0.626606  76.000000\n",
      "weighted avg   0.739938  0.657895  0.677806  76.000000\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.032686710357666016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcb941640f64de4ac20f9ee3f21dd4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 3 on model : pS-BERT_20%\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.885714  0.563636  0.688889  55.000000\n",
      "requirement    0.400000  0.800000  0.533333  20.000000\n",
      "accuracy       0.626667  0.626667  0.626667   0.626667\n",
      "macro avg      0.642857  0.681818  0.611111  75.000000\n",
      "weighted avg   0.756190  0.626667  0.647407  75.000000\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03318667411804199,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d9d87c50644fbd9592c62344b4f4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 4 on model : pS-BERT_20%\n",
      "\n",
      "               precision    recall  f1-score  support\n",
      "information    0.897436  0.636364  0.744681    55.00\n",
      "requirement    0.444444  0.800000  0.571429    20.00\n",
      "accuracy       0.680000  0.680000  0.680000     0.68\n",
      "macro avg      0.670940  0.718182  0.658055    75.00\n",
      "weighted avg   0.776638  0.680000  0.698480    75.00\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014818429946899414,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff564b1dba04e6187cfe7ddb3e79462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 5 on model : pS-BERT_20%\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.804348  0.660714  0.725490  56.000000\n",
      "requirement    0.344828  0.526316  0.416667  19.000000\n",
      "accuracy       0.626667  0.626667  0.626667   0.626667\n",
      "macro avg      0.574588  0.593515  0.571078  75.000000\n",
      "weighted avg   0.687936  0.626667  0.647255  75.000000\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_count = 1\n",
    "st_results = []\n",
    "avg_accuracy = []\n",
    "\n",
    "for subs in sorted(sub_folders):\n",
    "    test_dataset = get_dataset(subs)\n",
    "    \n",
    "    model_path = prefix + model_name + '/fold_' + str(fold_count)\n",
    "    ST_model = SetFitModel.from_pretrained(model_path)\n",
    "    \n",
    "    predicted_labels, y_test = evaluate_ST(test_dataset, ST_model)\n",
    "    \n",
    "    evaluation_results = classification_report(y_test, predicted_labels.tolist(), \n",
    "                                               target_names=list(map_labels.values()), \n",
    "                                               output_dict=True)\n",
    "    \n",
    "    avg_accuracy.append(get_accuracy(y_test, \n",
    "                                     predicted_labels.tolist()))\n",
    "\n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    st_results.append(report_df)\n",
    "    \n",
    "    print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "\n",
    "    fold_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5-folds</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weighted_avg</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro_avg</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_avg</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Precision  Recall  F1_score\n",
       "5-folds                                  \n",
       "weighted_avg       0.74    0.65      0.67\n",
       "macro_avg          0.63    0.66      0.62\n",
       "accuracy_avg       0.65    0.65      0.65"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_acc_score = round(np.mean(avg_accuracy), 2)\n",
    "weighted_avg, macro_avg = get_avg_report(st_results, folds=5)\n",
    "\n",
    "avg_scores = list([weighted_avg, macro_avg, (avg_acc_score, avg_acc_score, \n",
    "                                             avg_acc_score)])\n",
    "\n",
    "final_df = pd.DataFrame([x for x in avg_scores], \n",
    "                        columns=(['Precision', 'Recall', 'F1_score']),\n",
    "                        index=['weighted_avg','macro_avg', 'accuracy_avg'])\n",
    "\n",
    "final_df.rename_axis('5-folds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_model(model_name, prefix, model_path):\n",
    "    \n",
    "    embeddings_model = None\n",
    "    if model_name in ('LSTM_FT_pre-train', 'pLSTM_FT_pre-train'):\n",
    "        \n",
    "        # Load FastText pre trained embeddings\n",
    "        if not 'wiki-news-300d-1M-subword.vec' in os.listdir(prefix):\n",
    "            \n",
    "            print('Downloading FastText pretrained model for the first time...')\n",
    "            url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.vec.zip'\n",
    "            \n",
    "            wget.download(url, out=prefix)\n",
    "            with zipfile.ZipFile(prefix + 'wiki-news-300d-1M-subword.vec.zip', 'r') as zip_ref:\n",
    "                zip_ref.extractall(prefix)\n",
    "        \n",
    "        embeddings_model = KeyedVectors.load_word2vec_format(prefix + \n",
    "                                                          'wiki-news-300d-1M-subword.vec')\n",
    "        print('\\nFastText pretrained Model loaded...')        \n",
    "\n",
    "    elif model_name in ('LSTM_GLV_pre-train', 'pLSTM_GLV_pre-train'):\n",
    "        \n",
    "        # load pre trained Glove embeddings model\n",
    "        if not 'glove.6B.100d.txt' in os.listdir(prefix):\n",
    "            url = 'https://nlp.stanford.edu/data/glove.6B.zip'\n",
    "            wget.download(url, out=prefix)\n",
    "            \n",
    "            with zipfile.ZipFile(prefix + 'glove.6B.zip', 'r') as zip_ref:\n",
    "                zip_ref.extractall(prefix)\n",
    "            \n",
    "        embeddings_model = {}\n",
    "        with open(prefix + 'glove.6B.100d.txt','r') as f:\n",
    "            for line in f:\n",
    "                \n",
    "                split_line = line.split()\n",
    "                word = split_line[0]\n",
    "                embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "                embeddings_model[word] = embedding\n",
    "        \n",
    "        print('\\nPretrained Glove Model loaded...')        \n",
    "                \n",
    "    elif model_name in ('LSTM_GLV_custom', 'pLSTM_GLV_custom'):\n",
    "        \n",
    "        # load custom glove embeddings model\n",
    "        embeddings_model = Glove.load(model_path + '/glove_custom_100d.model')\n",
    "        print('\\nGlove custom pretrained Model loaded...')        \n",
    "      \n",
    "    elif model_name in ('LSTM_FT_custom', 'pLSTM_FT_custom'):\n",
    "        \n",
    "        # load custom FastText embeddings model\n",
    "        embeddings_model = KeyedVectors.load(model_path + '/fast_text.model')\n",
    "        print('\\nFastText custom Model loaded...')        \n",
    "    \n",
    "    return embeddings_model          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_index(string_data, wv, model_name):\n",
    "    \n",
    "    index_data = []\n",
    "    for word in string_data:\n",
    "        if word in wv:\n",
    "            try:\n",
    "                if 'GLV_custom' in model_name:\n",
    "                    index_data.append(wv[word])\n",
    "\n",
    "                else:\n",
    "                    index_data.append(wv.vocab[word].index)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return index_data\n",
    "\n",
    "def convert_data(train_sentences, test_sentences, modelf, model_name):\n",
    "    \n",
    "    train_data = []\n",
    "    i = 0\n",
    "    if 'GLV_custom' in model_name:\n",
    "        word_vectors = modelf.dictionary\n",
    "    else:\n",
    "        word_vectors = modelf.wv\n",
    "    \n",
    "    while i<len(train_sentences):\n",
    "        for seq in train_sentences[i]:\n",
    "            train_data.append(convert_data_to_index(train_sentences[i], word_vectors, \n",
    "                                                    model_name))\n",
    "            break\n",
    "\n",
    "        i+=1\n",
    "    \n",
    "    test_data = []\n",
    "    i = 0\n",
    "    while i<len(test_sentences):\n",
    "        for seq in test_sentences[i]:\n",
    "            test_data.append(convert_data_to_index(test_sentences[i], word_vectors, \n",
    "                                                   model_name))\n",
    "            break\n",
    "\n",
    "        i+=1\n",
    "    return train_data, test_data\n",
    "\n",
    "def pad_sequences(train_data, test_data):\n",
    "    \n",
    "    max_length_f = max([len(seq) for seq in train_data])   \n",
    "    test_padded = sequence.pad_sequences(test_data, maxlen=max_length_f, padding='pre')\n",
    "    return test_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset for testing\n",
    "fold_parent = './data/dronology_preprocess_five_folds/'\n",
    "#fold_parent = '../dataset/dronology_basic_data/'\n",
    "\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder))\n",
    "        \n",
    "\n",
    "map_labels = {0: 'information', 1: 'requirement'}\n",
    "prefix = './models/DL_models/LSTM_family/'\n",
    "# replace the value of 'model_name' with desired LSTM model's name to get resuts for the model\n",
    "# to trigger more LSTM models check the names in: model/LSTM_family. examples, LSTM_FT_custom. \n",
    "# put 'p' infront of the model name to couple our pre-processing pipeline\n",
    "model_name = 'pLSTM_FT_custom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FastText custom Model loaded...\n",
      "3/3 [==============================] - 1s 72ms/step\n",
      "\n",
      "Results for dataset fold number : 1 on model : pLSTM_FT_custom\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.825397  0.928571  0.873950  56.000000\n",
      "requirement    0.692308  0.450000  0.545455  20.000000\n",
      "accuracy       0.802632  0.802632  0.802632   0.802632\n",
      "macro avg      0.758852  0.689286  0.709702  76.000000\n",
      "weighted avg   0.790373  0.802632  0.787504  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "FastText custom Model loaded...\n",
      "3/3 [==============================] - 1s 75ms/step\n",
      "\n",
      "Results for dataset fold number : 2 on model : pLSTM_FT_custom\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.800000  0.928571  0.859504  56.000000\n",
      "requirement    0.636364  0.350000  0.451613  20.000000\n",
      "accuracy       0.776316  0.776316  0.776316   0.776316\n",
      "macro avg      0.718182  0.639286  0.655559  76.000000\n",
      "weighted avg   0.756938  0.776316  0.752164  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "FastText custom Model loaded...\n",
      "3/3 [==============================] - 1s 67ms/step\n",
      "\n",
      "Results for dataset fold number : 3 on model : pLSTM_FT_custom\n",
      "\n",
      "               precision    recall  f1-score  support\n",
      "information    0.812500  0.945455  0.873950     55.0\n",
      "requirement    0.727273  0.400000  0.516129     20.0\n",
      "accuracy       0.800000  0.800000  0.800000      0.8\n",
      "macro avg      0.769886  0.672727  0.695039     75.0\n",
      "weighted avg   0.789773  0.800000  0.778531     75.0\n",
      "--------------------------------------\n",
      "\n",
      "FastText custom Model loaded...\n",
      "3/3 [==============================] - 1s 80ms/step\n",
      "\n",
      "Results for dataset fold number : 4 on model : pLSTM_FT_custom\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.872340  0.745455  0.803922  55.000000\n",
      "requirement    0.500000  0.700000  0.583333  20.000000\n",
      "accuracy       0.733333  0.733333  0.733333   0.733333\n",
      "macro avg      0.686170  0.722727  0.693627  75.000000\n",
      "weighted avg   0.773050  0.733333  0.745098  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "FastText custom Model loaded...\n",
      "3/3 [==============================] - 1s 77ms/step\n",
      "\n",
      "Results for dataset fold number : 5 on model : pLSTM_FT_custom\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.781250  0.892857  0.833333  56.000000\n",
      "requirement    0.454545  0.263158  0.333333  19.000000\n",
      "accuracy       0.733333  0.733333  0.733333   0.733333\n",
      "macro avg      0.617898  0.578008  0.583333  75.000000\n",
      "weighted avg   0.698485  0.733333  0.706667  75.000000\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_count = 1\n",
    "results = []\n",
    "avg_accuracy = []\n",
    "\n",
    "for subs in sorted(sub_folders):\n",
    "    train_path = subs + '/train_' + 'fold_' + str(fold_count) + '.csv'\n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "    \n",
    "    test_df=pd.read_csv(test_path)\n",
    "    train_df=pd.read_csv(train_path)\n",
    "    \n",
    "    model_path = prefix + model_name + '/fold_' + str(fold_count)\n",
    "    \n",
    "    test_df['STR.REQ'] =  test_df['STR.REQ'].str.lower()\n",
    "    train_df['STR.REQ'] =  train_df['STR.REQ'].str.lower()\n",
    "    train_sentences = train_df['STR.REQ'].apply(str.split).values.tolist()\n",
    "    test_sentences = test_df['STR.REQ'].apply(str.split).values.tolist()\n",
    "\n",
    "    actual = test_df['class'].tolist()\n",
    "    \n",
    "    embeddings_model = get_embeddings_model(model_name, prefix, model_path)\n",
    "\n",
    "    if 'p' in model_name.split('_')[0]:\n",
    "        lstm_model = load_model(model_path + '/pLSTM.h5')\n",
    "    else:\n",
    "        lstm_model = load_model(model_path + '/LSTM.h5')\n",
    "    \n",
    "    train_data, test_data = convert_data(train_sentences, test_sentences, \n",
    "                                         embeddings_model, model_name)\n",
    "    test_padded = pad_sequences(train_data, test_data)\n",
    "    \n",
    "    test_padded = np.array(test_padded)              \n",
    "    \n",
    "    predictions = lstm_model.predict(test_padded)\n",
    "    sorted_predictions = (-predictions).argsort()\n",
    "    top_label_int = sorted_predictions[:, :1].flatten().tolist()\n",
    "    \n",
    "    evaluation_results = metrics.classification_report(actual, top_label_int, \n",
    "                                                       target_names=list(map_labels.values()),\n",
    "                                                       output_dict=True)\n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    results.append(report_df)\n",
    "    print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "    \n",
    "    avg_accuracy.append(get_accuracy(actual, top_label_int))\n",
    "    fold_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5-folds</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weighted_avg</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro_avg</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_avg</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Precision  Recall  F1_score\n",
       "5-folds                                  \n",
       "weighted_avg       0.76    0.77      0.75\n",
       "macro_avg          0.71    0.66      0.67\n",
       "accuracy_avg       0.77    0.77      0.77"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "avg_acc_score = round(np.mean(avg_accuracy), 2)\n",
    "weighted_avg, macro_avg = get_avg_report(results, folds=5)\n",
    "\n",
    "avg_scores = list([weighted_avg, macro_avg, (avg_acc_score, avg_acc_score, \n",
    "                                             avg_acc_score)])\n",
    "\n",
    "final_df = pd.DataFrame([x for x in avg_scores], \n",
    "                        columns=(['Precision', 'Recall', 'F1_score']),\n",
    "                        index=['weighted_avg','macro_avg', 'accuracy_avg'])\n",
    "\n",
    "final_df.rename_axis('5-folds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_label(ranges):\n",
    "    \"\"\"\n",
    "    returns the random label from the defined ranges of the labels\n",
    "    \"\"\"\n",
    "    temp=random.randint(1, ranges[-1][-1])\n",
    "    \n",
    "    for r in ranges:\n",
    "        if(temp>r[1] and temp<=r[-1]):\n",
    "            return r[0]\n",
    "    return None\n",
    "\n",
    "def get_ranges(df):\n",
    "    \"\"\"\n",
    "    predicts the random labels on the given test dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    csum = 0\n",
    "    ranges = []\n",
    "    total_tr = len(df)\n",
    "\n",
    "    for k, v in df['class'].value_counts().to_dict().items():\n",
    "\n",
    "        csum_old = csum\n",
    "        csum += round((v/total_tr) * 100,0)\n",
    "        #print (k,\"from\", csum_old, \"to\",csum)\n",
    "        ranges.append([k, csum_old, csum])\n",
    "    \n",
    "    r_out = []\n",
    "    for row in test_df.iterrows():\n",
    "        r3labels = []\n",
    "\n",
    "        while len(r3labels)!=1:\n",
    "            rl = get_random_label(ranges)\n",
    "            if not rl in r3labels:\n",
    "                r3labels.append(rl)\n",
    "\n",
    "        r_out.append([row[1]['issueid'], row[1]['class'], r3labels])\n",
    "\n",
    "    return ranges, r_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "fold_parent = './data/dronology_five_folds/'\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for fold number : 1\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.736842  0.750000  0.743363  56.000000\n",
      "requirement    0.263158  0.250000  0.256410  20.000000\n",
      "accuracy       0.618421  0.618421  0.618421   0.618421\n",
      "macro avg      0.500000  0.500000  0.499887  76.000000\n",
      "weighted avg   0.612188  0.618421  0.615217  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for fold number : 2\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.754386  0.767857  0.761062  56.000000\n",
      "requirement    0.315789  0.300000  0.307692  20.000000\n",
      "accuracy       0.644737  0.644737  0.644737   0.644737\n",
      "macro avg      0.535088  0.533929  0.534377  76.000000\n",
      "weighted avg   0.638966  0.644737  0.641754  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for fold number : 3\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.730769  0.690909  0.710280  55.000000\n",
      "requirement    0.260870  0.300000  0.279070  20.000000\n",
      "accuracy       0.586667  0.586667  0.586667   0.586667\n",
      "macro avg      0.495819  0.495455  0.494675  75.000000\n",
      "weighted avg   0.605463  0.586667  0.595291  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for fold number : 4\n",
      "\n",
      "               precision    recall  f1-score  support\n",
      "information    0.729167  0.636364  0.679612    55.00\n",
      "requirement    0.259259  0.350000  0.297872    20.00\n",
      "accuracy       0.560000  0.560000  0.560000     0.56\n",
      "macro avg      0.494213  0.493182  0.488742    75.00\n",
      "weighted avg   0.603858  0.560000  0.577815    75.00\n",
      "--------------------------------------\n",
      "\n",
      "Results for fold number : 5\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.750000  0.803571  0.775862  56.000000\n",
      "requirement    0.266667  0.210526  0.235294  19.000000\n",
      "accuracy       0.653333  0.653333  0.653333   0.653333\n",
      "macro avg      0.508333  0.507049  0.505578  75.000000\n",
      "weighted avg   0.627556  0.653333  0.638918  75.000000\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_count = 1\n",
    "for subs in sorted(sub_folders):\n",
    "    \n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    ranges, r_out = get_ranges(test_df)\n",
    "    \n",
    "    random_out = pd.DataFrame()\n",
    "    random_out['issueid'] = [i[0] for i in r_out]\n",
    "    random_out['class'] = [i[1] for i in r_out]\n",
    "    random_out['top_label'] = [i[2][0] for i in r_out]\n",
    "    evaluation_results = classification_report(random_out['class'], random_out['top_label'], \n",
    "                                               target_names=list(map_labels.values()), \n",
    "                                               output_dict=True)\n",
    "    \n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    print('\\nResults for fold number :',fold_count)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "    \n",
    "    fold_count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
