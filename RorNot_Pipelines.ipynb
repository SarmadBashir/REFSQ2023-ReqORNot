{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "65409155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from typing import Dict\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from datasets import load_dataset, logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import XLNetForSequenceClassification, RobertaForSequenceClassification\n",
    "from transformers import XLMRobertaForSequenceClassification, DistilBertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, XLMRobertaTokenizer, DistilBertTokenizer, XLNetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ea064",
   "metadata": {},
   "source": [
    "### Common Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "9d7c8fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_report(results, folds):\n",
    "    \n",
    "    \"\"\"\n",
    "    function takes the input of predicted model results on five folds and returns\n",
    "    average of weighted and macro Precision, Recall, F-1 \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    weighted_precision = []\n",
    "    weighted_recall = []\n",
    "    weighted_f1 = []\n",
    "    \n",
    "    macro_precision = []\n",
    "    macro_recall = []\n",
    "    macro_f1 = []\n",
    "    \n",
    "    for result_df in results:                        \n",
    "        res_rows = result_df.tail(3)\n",
    "\n",
    "        precision_scores =  res_rows['precision'].tolist()\n",
    "        recall_scores =  res_rows['recall'].tolist()\n",
    "        f1_scores =  res_rows['f1-score'].tolist()\n",
    "\n",
    "        precision_macro_avg =  precision_scores[1]\n",
    "        precision_weighted_avg = precision_scores[2]\n",
    "\n",
    "        recall_macro_avg =  recall_scores[1]\n",
    "        recall_weighted_avg = recall_scores[2]\n",
    "\n",
    "        fl_accuracy = f1_scores[0]\n",
    "        f1_scores_macro_avg =  f1_scores[1]\n",
    "        f1_scores_weighted_avg = f1_scores[2]\n",
    "                \n",
    "        weighted_precision.append(precision_weighted_avg)\n",
    "        weighted_recall.append(recall_weighted_avg)\n",
    "        weighted_f1.append(f1_scores_weighted_avg)\n",
    "        \n",
    "        macro_precision.append(precision_macro_avg)\n",
    "        macro_recall.append(recall_macro_avg)\n",
    "        macro_f1.append(f1_scores_macro_avg)\n",
    "                \n",
    "    weighted_average = round(sum(weighted_precision) / folds, 2), round(sum(weighted_recall) / folds, 2), round(sum(weighted_f1) / folds, 2)\n",
    "    macro_average = round(sum(macro_precision) / folds, 2), round(sum(macro_recall) / folds, 2), round(sum(macro_f1) / folds, 2)\n",
    "            \n",
    "    return weighted_average, macro_average\n",
    "\n",
    "def get_accuracy(y_actual, y_predicted):\n",
    "    \"\"\"\n",
    "    function takes the actual and predicted labels to return\n",
    "    the accuracy per fold\n",
    "    \n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for index in zip(y_actual, y_predicted):\n",
    "        \n",
    "        if index[0] == index[1]:\n",
    "                count += 1\n",
    "    topk_acc = round(count / len(y_actual), 2)\n",
    "    return topk_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3493c90d",
   "metadata": {},
   "source": [
    "### ML alogrithms Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d35c6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ML_model_files(model_name, model_path, pca):\n",
    "    \n",
    "    \"\"\"\n",
    "    function load the ML models relevant files based \n",
    "    on the parameters given\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ML_model = pickle.load(open(model_path + '/'+ model_name + '.pickle', 'rb'))\n",
    "    if pca:\n",
    "        pca_vectorizer = pickle.load(open(model_path + 'pca_vectorizer.pickle', \"rb\"))\n",
    "    else:\n",
    "        pca_vectorizer = None\n",
    "    tfidf_vectorizer = pickle.load(open(model_path + 'tfidf_vectorizer.pickle', \"rb\"))\n",
    "    \n",
    "    return ML_model, pca_vectorizer, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ac95ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset for testing\n",
    "fold_parent = './data/dronology_five_folds/'\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ee9e8524",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'SVM'\n",
    "PCA = True\n",
    "map_labels = {0: 'information', 1: 'requirement'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "084b6fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 1 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.823529  1.000000  0.903226  56.000000\n",
      "requirement    1.000000  0.400000  0.571429  20.000000\n",
      "accuracy       0.842105  0.842105  0.842105   0.842105\n",
      "macro avg      0.911765  0.700000  0.737327  76.000000\n",
      "weighted avg   0.869969  0.842105  0.815911  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 2 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.787879  0.928571  0.852459  56.000000\n",
      "requirement    0.600000  0.300000  0.400000  20.000000\n",
      "accuracy       0.763158  0.763158  0.763158   0.763158\n",
      "macro avg      0.693939  0.614286  0.626230  76.000000\n",
      "weighted avg   0.738437  0.763158  0.733391  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 3 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score  support\n",
      "information    0.768116  0.963636  0.854839    55.00\n",
      "requirement    0.666667  0.200000  0.307692    20.00\n",
      "accuracy       0.760000  0.760000  0.760000     0.76\n",
      "macro avg      0.717391  0.581818  0.581266    75.00\n",
      "weighted avg   0.741063  0.760000  0.708933    75.00\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 4 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.805970  0.981818  0.885246  55.000000\n",
      "requirement    0.875000  0.350000  0.500000  20.000000\n",
      "accuracy       0.813333  0.813333  0.813333   0.813333\n",
      "macro avg      0.840485  0.665909  0.692623  75.000000\n",
      "weighted avg   0.824378  0.813333  0.782514  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 5 on model : SVM\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.776119  0.928571  0.845528  56.000000\n",
      "requirement    0.500000  0.210526  0.296296  19.000000\n",
      "accuracy       0.746667  0.746667  0.746667   0.746667\n",
      "macro avg      0.638060  0.569549  0.570912  75.000000\n",
      "weighted avg   0.706169  0.746667  0.706390  75.000000\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# load test data & make prediction\n",
    "\n",
    "ml_results = []\n",
    "avg_accuracy = []\n",
    "fold_count = 1\n",
    "\n",
    "for subs in sorted(sub_folders):\n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "    \n",
    "    df_test = pd.read_csv(test_path)\n",
    "    df_test['STR.REQ'] = df_test['STR.REQ'].str.lower()\n",
    "    X_test = df_test['STR.REQ']\n",
    "    y_test = df_test['class']\n",
    "    \n",
    "    model_path = './models/ML_models/' + model_name + '/fold_' + str(fold_count) + '/'\n",
    "    ML_model, pca_vectorizer, tfidf_vectorizer = load_ML_model_files(model_name, model_path, PCA)\n",
    "\n",
    "    tfidf_vecs = tfidf_vectorizer.transform(X_test)\n",
    "    normalized_tfidf = normalize(tfidf_vecs)\n",
    "\n",
    "    test_vecs = pca_vectorizer.transform(normalized_tfidf.toarray())\n",
    "    predicted_labels = ML_model.predict(test_vecs)\n",
    "    \n",
    "    evaluation_results = classification_report(y_test.tolist(), predicted_labels.tolist(), \n",
    "                                               target_names=list(map_labels.values()), \n",
    "                                               output_dict=True)\n",
    "    \n",
    "    avg_accuracy.append(get_accuracy(y_test.tolist(), predicted_labels.tolist()))\n",
    "    \n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    ml_results.append(report_df)\n",
    "    \n",
    "    print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "    \n",
    "    fold_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f5919c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5-folds</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weighted_avg</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro_avg</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_avg</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Precision  Recall  F1_score\n",
       "5-folds                                  \n",
       "weighted_avg       0.78    0.79      0.75\n",
       "macro_avg          0.76    0.63      0.64\n",
       "accuracy_avg       0.78    0.78      0.78"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average results of ML pipeline\n",
    "\n",
    "avg_acc_score = round(np.mean(avg_accuracy), 2)\n",
    "weighted_avg, macro_avg = get_avg_report(ml_results, folds=5)\n",
    "\n",
    "avg_scores = list([weighted_avg, macro_avg, (avg_acc_score, avg_acc_score, avg_acc_score)])\n",
    "\n",
    "final_df = pd.DataFrame([x for x in avg_scores], columns=(['Precision', 'Recall', 'F1_score']),\n",
    "                      index=['weighted_avg','macro_avg', 'accuracy_avg'])\n",
    "\n",
    "final_df.rename_axis('5-folds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdae2819",
   "metadata": {},
   "source": [
    "### BERT Family Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ed68a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer(model_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    loads and returns the relevant tokenizer for passed parameter BERT model name\n",
    "    \n",
    "    \"\"\"\n",
    "    if model_name in ('BERT_base_uncased', \n",
    "                      'pBERT_base_uncased'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",\n",
    "                                                  do_lower_case=True)\n",
    "                \n",
    "    elif model_name in ('BERT_base_cased',\n",
    "                        'pBERT_base_cased'):\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    \n",
    "    elif model_name in ('pXLNet_base', \n",
    "                        'XLNet_base'):\n",
    "        tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "    \n",
    "    elif model_name in ('SciBERT_uncased', \n",
    "                        'pSciBERT_uncased'):\n",
    "        tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', \n",
    "                                                  do_lower_case=True)\n",
    "    \n",
    "    elif model_name in ('pRoBERTa_base', \n",
    "                        'RoBERTa_base'):\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "    elif model_name in ('DisBERT_base_cased', \n",
    "                        'pDisBERT_base_cased'):\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "    \n",
    "    elif model_name in ('DisBERT_base_uncased', \n",
    "                        'pDisBERT_base_uncased'):\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    else:\n",
    "        #'pXRBERT_base', 'XRBERT_base'\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    \n",
    "    return tokenizer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "40443237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_BERT_model(model_name, model_path):\n",
    "    \"\"\"\n",
    "    loads and returns the BERT model based on the model name and path parameters\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if model_name in ('BERT_base_uncased', 'pBERT_base_cased',\n",
    "                      'pBERT_base_uncased', 'BERT_base_cased',\n",
    "                      'SciBERT_uncased', 'pSciBERT_uncased'\n",
    "                     ):\n",
    "        model = BertForSequenceClassification.from_pretrained(model_path)                \n",
    "    elif model_name in ('pXLNet_base', \n",
    "                        'XLNet_base'\n",
    "                       ):\n",
    "        model = XLNetForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    elif model_name in ('pRoBERTa_base', \n",
    "                        'RoBERTa_base'\n",
    "                       ):\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    elif model_name in ('DisBERT_base_cased', 'DisBERT_base_uncased',\n",
    "                        'pDisBERT_base_cased', 'pDisBERT_base_uncased'\n",
    "                       ):\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(model_path)    \n",
    "    \n",
    "    else:\n",
    "        #'pXRBERT_base', 'XRBERT_base'\n",
    "        model = XLMRobertaForSequenceClassification.from_pretrained(model_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "73b82e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model name to test the pipeline\n",
    "\n",
    "map_labels = {0: 'information', 1: 'requirement'}\n",
    "\n",
    "prefix = './models/DL_models/'\n",
    "model_name = 'DisBERT_base_uncased'\n",
    "\n",
    "fold_parent = './data/dronology_five_folds/'\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder))\n",
    "\n",
    "tokenizer = load_tokenizer(model_name)\n",
    "MAX_SEQ_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "47180049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "edcad63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 1 on model : DisBERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.981132  0.928571  0.954128  56.000000\n",
      "requirement    0.826087  0.950000  0.883721  20.000000\n",
      "accuracy       0.934211  0.934211  0.934211   0.934211\n",
      "macro avg      0.903610  0.939286  0.918925  76.000000\n",
      "weighted avg   0.940331  0.934211  0.935600  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 2 on model : DisBERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.833333  0.892857  0.862069  56.000000\n",
      "requirement    0.625000  0.500000  0.555556  20.000000\n",
      "accuracy       0.789474  0.789474  0.789474   0.789474\n",
      "macro avg      0.729167  0.696429  0.708812  76.000000\n",
      "weighted avg   0.778509  0.789474  0.781408  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 3 on model : DisBERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score  support\n",
      "information    0.942308  0.890909  0.915888    55.00\n",
      "requirement    0.739130  0.850000  0.790698    20.00\n",
      "accuracy       0.880000  0.880000  0.880000     0.88\n",
      "macro avg      0.840719  0.870455  0.853293    75.00\n",
      "weighted avg   0.888127  0.880000  0.882504    75.00\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 4 on model : DisBERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.944444  0.927273  0.935780  55.000000\n",
      "requirement    0.809524  0.850000  0.829268  20.000000\n",
      "accuracy       0.906667  0.906667  0.906667   0.906667\n",
      "macro avg      0.876984  0.888636  0.882524  75.000000\n",
      "weighted avg   0.908466  0.906667  0.907377  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for dataset fold number : 5 on model : DisBERT_base_uncased\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.809524  0.910714  0.857143  56.000000\n",
      "requirement    0.583333  0.368421  0.451613  19.000000\n",
      "accuracy       0.773333  0.773333  0.773333   0.773333\n",
      "macro avg      0.696429  0.639568  0.654378  75.000000\n",
      "weighted avg   0.752222  0.773333  0.754409  75.000000\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_count = 1\n",
    "results = []\n",
    "avg_accuracy = []\n",
    "for subs in sorted(sub_folders):\n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "    \n",
    "    df_test = pd.read_csv(test_path)\n",
    "    selected_test = df_test[['STR.REQ','class']]\n",
    "\n",
    "    test_sequences = selected_test['STR.REQ'].tolist()\n",
    "\n",
    "    test_encodings = tokenizer(test_sequences, truncation=True, \n",
    "                               padding=True, \n",
    "                               max_length=MAX_SEQ_LENGTH, \n",
    "                               return_tensors=\"pt\")\n",
    "    # load model\n",
    "    model_path = glob.glob(prefix + model_name + '/fold_' + str(fold_count) + '/*')[0]\n",
    "    bert_model = load_BERT_model(model_name, model_path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = bert_model(**test_encodings).logits\n",
    "\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    evaluation_results = classification_report(selected_test['class'].tolist(), \n",
    "                                               predictions.tolist(), \n",
    "                                               target_names=list(map_labels.values()), \n",
    "                                               output_dict=True)\n",
    "    \n",
    "    avg_accuracy.append(get_accuracy(selected_test['class'].tolist(), \n",
    "                                     predictions.tolist()))\n",
    "\n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    results.append(report_df)\n",
    "    \n",
    "    print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "    \n",
    "    fold_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "9067d829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5-folds</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weighted_avg</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro_avg</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_avg</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Precision  Recall  F1_score\n",
       "5-folds                                  \n",
       "weighted_avg       0.85    0.86      0.85\n",
       "macro_avg          0.81    0.81      0.80\n",
       "accuracy_avg       0.86    0.86      0.86"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average results of BERT model\n",
    "\n",
    "avg_acc_score = round(np.mean(avg_accuracy), 2)\n",
    "weighted_avg, macro_avg = get_avg_report(results, folds=5)\n",
    "\n",
    "avg_scores = list([weighted_avg, macro_avg, (avg_acc_score, avg_acc_score, \n",
    "                                             avg_acc_score)])\n",
    "\n",
    "final_df = pd.DataFrame([x for x in avg_scores], \n",
    "                        columns=(['Precision', 'Recall', 'F1_score']),\n",
    "                        index=['weighted_avg','macro_avg', 'accuracy_avg'])\n",
    "\n",
    "final_df.rename_axis('5-folds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743605db",
   "metadata": {},
   "source": [
    "### Sentence-BERT_Few shot pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "43af6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "    \n",
    "    \"\"\"\n",
    "    load and return the dataset in the format fine-tuned few shot sentence-BERT \n",
    "    expects\n",
    "    \n",
    "    \"\"\"\n",
    "    dataset = load_dataset(path)\n",
    "    test_dataset = dataset['test']\n",
    "    \n",
    "    return test_dataset\n",
    "\n",
    "\n",
    "def _apply_column_mapping(dataset:dataset, column_mapping: Dict[str, str]):\n",
    "    \n",
    "    \"\"\"\n",
    "    apply the column mapping required for the loaded dataset\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "        dataset = dataset.rename_columns(\n",
    "            {\n",
    "                **column_mapping,\n",
    "                **{col: f\"feat_{col}\" for col in dataset.column_names if col not in column_mapping},\n",
    "            }\n",
    "        )\n",
    "        dset_format = dataset.format\n",
    "        dataset = dataset.with_format(\n",
    "            type=dset_format[\"type\"],\n",
    "            columns=dataset.column_names,\n",
    "            output_all_columns=dset_format[\"output_all_columns\"],\n",
    "            **dset_format[\"format_kwargs\"],\n",
    "        )\n",
    "        return dataset\n",
    "\n",
    "def evaluate_ST(test_data, Sent_tf_model):\n",
    "    \n",
    "    \"\"\"\n",
    "    load and evaluate the Sentence-BERT model on the given test dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    eval_dataset = _apply_column_mapping(dataset=test_data, \n",
    "                                         column_mapping={\"STR.REQ\": \"text\", \"class\": \"label\"})   \n",
    "    x_test = eval_dataset[\"text\"]\n",
    "    y_test = eval_dataset[\"label\"]\n",
    "\n",
    "    predicted_labels = Sent_tf_model.predict(x_test)\n",
    "    \n",
    "    return predicted_labels, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "fb184726",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pS-BERT_20%'\n",
    "\n",
    "prefix = './models/DL_models/'\n",
    "fold_parent = './data/dronology_preprocess_five_folds/'\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "a72c7a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012885093688964844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad483ae21ec45cfb77eb3428fcea10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 1 on model : pS-BERT_20%\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.840909  0.660714  0.740000  56.000000\n",
      "requirement    0.406250  0.650000  0.500000  20.000000\n",
      "accuracy       0.657895  0.657895  0.657895   0.657895\n",
      "macro avg      0.623580  0.655357  0.620000  76.000000\n",
      "weighted avg   0.726525  0.657895  0.676842  76.000000\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03174757957458496,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17810a2f7dec41a5a6c76ef2dec0f43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 2 on model : pS-BERT_20%\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.857143  0.642857  0.734694  56.000000\n",
      "requirement    0.411765  0.700000  0.518519  20.000000\n",
      "accuracy       0.657895  0.657895  0.657895   0.657895\n",
      "macro avg      0.634454  0.671429  0.626606  76.000000\n",
      "weighted avg   0.739938  0.657895  0.677806  76.000000\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03218412399291992,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a24ba9f9ad47c2a887b895aa38b014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 3 on model : pS-BERT_20%\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.885714  0.563636  0.688889  55.000000\n",
      "requirement    0.400000  0.800000  0.533333  20.000000\n",
      "accuracy       0.626667  0.626667  0.626667   0.626667\n",
      "macro avg      0.642857  0.681818  0.611111  75.000000\n",
      "weighted avg   0.756190  0.626667  0.647407  75.000000\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03212285041809082,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a755172d5f3946a38ff81572150941fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 4 on model : pS-BERT_20%\n",
      "\n",
      "               precision    recall  f1-score  support\n",
      "information    0.897436  0.636364  0.744681    55.00\n",
      "requirement    0.444444  0.800000  0.571429    20.00\n",
      "accuracy       0.680000  0.680000  0.680000     0.68\n",
      "macro avg      0.670940  0.718182  0.658055    75.00\n",
      "weighted avg   0.776638  0.680000  0.698480    75.00\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03218197822570801,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc34033f676949cc97b536cfe21c880a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for dataset fold number : 5 on model : pS-BERT_20%\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.804348  0.660714  0.725490  56.000000\n",
      "requirement    0.344828  0.526316  0.416667  19.000000\n",
      "accuracy       0.626667  0.626667  0.626667   0.626667\n",
      "macro avg      0.574588  0.593515  0.571078  75.000000\n",
      "weighted avg   0.687936  0.626667  0.647255  75.000000\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_count = 1\n",
    "st_results = []\n",
    "avg_accuracy = []\n",
    "\n",
    "for subs in sorted(sub_folders):\n",
    "    test_dataset = get_dataset(subs)\n",
    "    \n",
    "    model_path = prefix + model_name + '/fold_' + str(fold_count)\n",
    "    ST_model = SetFitModel.from_pretrained(model_path)\n",
    "    \n",
    "    predicted_labels, y_test = evaluate(test_dataset, ST_model)\n",
    "    \n",
    "    evaluation_results = classification_report(y_test, predicted_labels.tolist(), \n",
    "                                               target_names=list(map_labels.values()), \n",
    "                                               output_dict=True)\n",
    "    \n",
    "    avg_accuracy.append(get_accuracy(y_test, \n",
    "                                     predicted_labels.tolist()))\n",
    "\n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    st_results.append(report_df)\n",
    "    \n",
    "    print('\\nResults for dataset fold number :',fold_count, 'on model :', model_name)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "\n",
    "    fold_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "a7eef2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5-folds</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weighted_avg</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro_avg</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy_avg</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Precision  Recall  F1_score\n",
       "5-folds                                  \n",
       "weighted_avg       0.74    0.65      0.67\n",
       "macro_avg          0.63    0.66      0.62\n",
       "accuracy_avg       0.65    0.65      0.65"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_acc_score = round(np.mean(avg_accuracy), 2)\n",
    "weighted_avg, macro_avg = get_avg_report(st_results, folds=5)\n",
    "\n",
    "avg_scores = list([weighted_avg, macro_avg, (avg_acc_score, avg_acc_score, \n",
    "                                             avg_acc_score)])\n",
    "\n",
    "final_df = pd.DataFrame([x for x in avg_scores], \n",
    "                        columns=(['Precision', 'Recall', 'F1_score']),\n",
    "                        index=['weighted_avg','macro_avg', 'accuracy_avg'])\n",
    "\n",
    "final_df.rename_axis('5-folds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e2599",
   "metadata": {},
   "source": [
    "### Random Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a5087d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_label(ranges):\n",
    "    \"\"\"\n",
    "    returns the random label from the defined ranges of the labels\n",
    "    \"\"\"\n",
    "    temp=random.randint(1, ranges[-1][-1])\n",
    "    \n",
    "    for r in ranges:\n",
    "        if(temp>r[1] and temp<=r[-1]):\n",
    "            return r[0]\n",
    "    return None\n",
    "\n",
    "def get_ranges(df):\n",
    "    \"\"\"\n",
    "    predicts the random labels on the given test dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    csum = 0\n",
    "    ranges = []\n",
    "    total_tr = len(df)\n",
    "\n",
    "    for k, v in df['class'].value_counts().to_dict().items():\n",
    "\n",
    "        csum_old = csum\n",
    "        csum += round((v/total_tr) * 100,0)\n",
    "        #print (k,\"from\", csum_old, \"to\",csum)\n",
    "        ranges.append([k, csum_old, csum])\n",
    "    \n",
    "    r_out = []\n",
    "    for row in test_df.iterrows():\n",
    "        r3labels = []\n",
    "\n",
    "        while len(r3labels)!=1:\n",
    "            rl = get_random_label(ranges)\n",
    "            if not rl in r3labels:\n",
    "                r3labels.append(rl)\n",
    "\n",
    "        r_out.append([row[1]['issueid'], row[1]['class'], r3labels])\n",
    "\n",
    "    return ranges, r_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9039615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "fold_parent = './data/dronology_five_folds/'\n",
    "\n",
    "sub_folders = []\n",
    "for folder in os.listdir(fold_parent):\n",
    "    if 'fold' in folder: \n",
    "        sub_folders.append(os.path.join(fold_parent, folder)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "85ebc964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for fold number : 1\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.736842  0.750000  0.743363  56.000000\n",
      "requirement    0.263158  0.250000  0.256410  20.000000\n",
      "accuracy       0.618421  0.618421  0.618421   0.618421\n",
      "macro avg      0.500000  0.500000  0.499887  76.000000\n",
      "weighted avg   0.612188  0.618421  0.615217  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for fold number : 2\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.754386  0.767857  0.761062  56.000000\n",
      "requirement    0.315789  0.300000  0.307692  20.000000\n",
      "accuracy       0.644737  0.644737  0.644737   0.644737\n",
      "macro avg      0.535088  0.533929  0.534377  76.000000\n",
      "weighted avg   0.638966  0.644737  0.641754  76.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for fold number : 3\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.730769  0.690909  0.710280  55.000000\n",
      "requirement    0.260870  0.300000  0.279070  20.000000\n",
      "accuracy       0.586667  0.586667  0.586667   0.586667\n",
      "macro avg      0.495819  0.495455  0.494675  75.000000\n",
      "weighted avg   0.605463  0.586667  0.595291  75.000000\n",
      "--------------------------------------\n",
      "\n",
      "Results for fold number : 4\n",
      "\n",
      "               precision    recall  f1-score  support\n",
      "information    0.729167  0.636364  0.679612    55.00\n",
      "requirement    0.259259  0.350000  0.297872    20.00\n",
      "accuracy       0.560000  0.560000  0.560000     0.56\n",
      "macro avg      0.494213  0.493182  0.488742    75.00\n",
      "weighted avg   0.603858  0.560000  0.577815    75.00\n",
      "--------------------------------------\n",
      "\n",
      "Results for fold number : 5\n",
      "\n",
      "               precision    recall  f1-score    support\n",
      "information    0.750000  0.803571  0.775862  56.000000\n",
      "requirement    0.266667  0.210526  0.235294  19.000000\n",
      "accuracy       0.653333  0.653333  0.653333   0.653333\n",
      "macro avg      0.508333  0.507049  0.505578  75.000000\n",
      "weighted avg   0.627556  0.653333  0.638918  75.000000\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "fold_count = 1\n",
    "for subs in sorted(sub_folders):\n",
    "    \n",
    "    test_path = subs + '/test_' + 'fold_' + str(fold_count) + '.csv'\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    ranges, r_out = get_ranges(test_df)\n",
    "    \n",
    "    random_out = pd.DataFrame()\n",
    "    random_out['issueid'] = [i[0] for i in r_out]\n",
    "    random_out['class'] = [i[1] for i in r_out]\n",
    "    random_out['top_label'] = [i[2][0] for i in r_out]\n",
    "    evaluation_results = classification_report(random_out['class'], random_out['top_label'], \n",
    "                                               target_names=list(map_labels.values()), \n",
    "                                               output_dict=True)\n",
    "    \n",
    "    report_df = pd.DataFrame(evaluation_results).transpose()\n",
    "    print('\\nResults for fold number :',fold_count)\n",
    "    print('\\n',report_df)\n",
    "    print('--------------------------------------')\n",
    "    \n",
    "    fold_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e5e0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
